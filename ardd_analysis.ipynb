{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e71c29b-6941-4b57-9251-4fb0f33ad938",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import calendar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a10a78-1caa-41a4-8eae-ccbc80da0e26",
   "metadata": {},
   "source": [
    "# Reading in Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72ad2412-f100-4625-80b4-f5b25cd30b46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ardd_fatal_crashes.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jesss\\Downloads\\ardd_analysis.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jesss/Downloads/ardd_analysis.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Read the data into a Pandas DataFrame\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jesss/Downloads/ardd_analysis.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m ardd_fatal_crashes_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_excel(\u001b[39m'\u001b[39m\u001b[39mardd_fatal_crashes.xlsx\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jesss/Downloads/ardd_analysis.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m ardd_fatal_crashes_df\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\jesss\\anaconda3\\envs\\PythonData\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:478\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    477\u001b[0m     should_close \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 478\u001b[0m     io \u001b[39m=\u001b[39m ExcelFile(io, storage_options\u001b[39m=\u001b[39mstorage_options, engine\u001b[39m=\u001b[39mengine)\n\u001b[0;32m    479\u001b[0m \u001b[39melif\u001b[39;00m engine \u001b[39mand\u001b[39;00m engine \u001b[39m!=\u001b[39m io\u001b[39m.\u001b[39mengine:\n\u001b[0;32m    480\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEngine should not be specified when passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jesss\\anaconda3\\envs\\PythonData\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1496\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1494\u001b[0m     ext \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxls\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1495\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1496\u001b[0m     ext \u001b[39m=\u001b[39m inspect_excel_format(\n\u001b[0;32m   1497\u001b[0m         content_or_path\u001b[39m=\u001b[39mpath_or_buffer, storage_options\u001b[39m=\u001b[39mstorage_options\n\u001b[0;32m   1498\u001b[0m     )\n\u001b[0;32m   1499\u001b[0m     \u001b[39mif\u001b[39;00m ext \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1500\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1501\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExcel file format cannot be determined, you must specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1502\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39man engine manually.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1503\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\jesss\\anaconda3\\envs\\PythonData\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1371\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1368\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(content_or_path, \u001b[39mbytes\u001b[39m):\n\u001b[0;32m   1369\u001b[0m     content_or_path \u001b[39m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1371\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m   1372\u001b[0m     content_or_path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m, storage_options\u001b[39m=\u001b[39mstorage_options, is_text\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1373\u001b[0m ) \u001b[39mas\u001b[39;00m handle:\n\u001b[0;32m   1374\u001b[0m     stream \u001b[39m=\u001b[39m handle\u001b[39m.\u001b[39mhandle\n\u001b[0;32m   1375\u001b[0m     stream\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jesss\\anaconda3\\envs\\PythonData\\Lib\\site-packages\\pandas\\io\\common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n\u001b[0;32m    869\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[0;32m    871\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ardd_fatal_crashes.xlsx'"
     ]
    }
   ],
   "source": [
    "# Read the data into a Pandas DataFrame\n",
    "ardd_fatal_crashes_df = pd.read_excel('ardd_fatal_crashes.xlsx')\n",
    "ardd_fatal_crashes_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed7ca2-68cb-4617-b1b8-c21478b7e529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the data into a Pandas DataFrame\n",
    "ardd_fatalities_df = pd.read_excel('ardd_fatalities.xlsx')\n",
    "ardd_fatalities_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802bf2d8-ce3c-4f5c-aa08-a7028a7961b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get a brief summary of the ardd_fatal_crashes DataFrame.\n",
    "ardd_fatal_crashes_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca92c73-6a5d-4362-a913-10407422e0de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get a brief summary of the ardd_fatalities DataFrame.\n",
    "ardd_fatalities_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9017ec2-cf3a-4e74-9ec2-8d7fba38ed38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the ardd_fatal_crashes_df columns.\n",
    "ardd_fatal_crashes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db828cbd-a4d7-4f97-ba2a-0f5e18ce1dbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the ardd_fatalities_df columns.\n",
    "ardd_fatalities_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffadb93f-13df-44e9-903a-7fc9ee89fc69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Given Similarity in both datasets, drop common columns \n",
    "ardd_fatalities_df = ardd_fatalities_df.drop(['State', 'Month', 'Year', 'Dayweek', 'Time', 'Crash Type', 'Bus Involvement', 'Heavy Rigid Truck Involvement',\n",
    "       'Articulated Truck Involvement', 'Speed Limit', 'National Remoteness Areas', 'SA4 Name 2021',\n",
    "       'National LGA Name 2021', 'National Road Type', 'Christmas Period',\n",
    "       'Easter Period', 'Day of week', 'Time of day'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d855ce81-fb9a-46a5-a4c2-5af2dec7fcbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the ardd_fatalities_df columns.\n",
    "ardd_fatalities_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9cdc11-17b0-4522-83b9-e9d003b001d6",
   "metadata": {},
   "source": [
    "# Merging Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4a8fa9-d5a8-4c79-9f12-b41ba1c036e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge the dataframes\n",
    "merged_df = pd.merge(ardd_fatal_crashes_df, ardd_fatalities_df, on='Crash ID', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c855e3-2884-4d3a-97b7-bb34f0c80f59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the merged_df columns.\n",
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6ca5d-18cb-44b0-94d7-e84bbd66f7c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get a brief summary of the merged DataFrame.\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d40b8c5-9f85-432c-a03f-0d39f2c09558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 24)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4598765d-16fc-4747-810f-f872a089f5ef",
   "metadata": {},
   "source": [
    "# Data Cleaning - Removing unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e9efc7-3f03-4eb1-b577-3f363c31be70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store unique values for each column\n",
    "unique_values = {}\n",
    "\n",
    "# Loop through the columns and find unique values\n",
    "for column in merged_df.columns:\n",
    "    unique_values[column] = merged_df[column].unique()\n",
    "\n",
    "# Display unique values for each column\n",
    "for column, values in unique_values.items():\n",
    "    print(f'Column: {column}')\n",
    "    print(values)\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be0dc07-26e8-4378-9165-bf23d4b194d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the list of special values you want to count\n",
    "special_values = ['-9', 'nan', '<40', 'Unspecified', 'Unknown', 'Undetermined', 'Other/-9', 'U', 'M ']\n",
    "\n",
    "# Create an empty dictionary to store the counts\n",
    "special_value_counts = {}\n",
    "\n",
    "# Iterate through the columns and count the special values\n",
    "for column in merged_df.columns:\n",
    "    column_counts = {}\n",
    "    if merged_df[column].dtype == 'object':\n",
    "        for special_value in special_values:\n",
    "            if special_value == 'nan':\n",
    "                count = merged_df[column].isna().sum()\n",
    "            elif merged_df[column].dtype == 'str':\n",
    "                count = (merged_df[column].str.strip() == special_value).sum()\n",
    "            else:\n",
    "                count = (merged_df[column] == special_value).sum()\n",
    "            column_counts[special_value] = count\n",
    "        special_value_counts[column] = column_counts\n",
    "\n",
    "# Display the counts\n",
    "for column, counts in special_value_counts.items():\n",
    "    print(f'Column: {column}')\n",
    "    for value, count in counts.items():\n",
    "        print(f'{value}: {count}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffce8478-4f31-4f84-a5d1-218a5d2615c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the list of special values you want to check for\n",
    "special_values = ['-9', 'nan', '<40', 'Unspecified', 'Unknown', 'Undetermined', 'Other/-9', 'U', 'M ']\n",
    "\n",
    "# Create a boolean mask to identify rows containing special values\n",
    "special_value_mask = merged_df.isin(special_values)\n",
    "\n",
    "# Count the rows with special values in each column\n",
    "rows_with_special_values = special_value_mask.any(axis=1)\n",
    "\n",
    "# Print the total number of rows with special values\n",
    "total_rows_with_special_values = rows_with_special_values.sum()\n",
    "print(f'Total rows with special values: {total_rows_with_special_values}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aed3f19-c67a-40c5-b1bc-394c45577040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the list of special values you want to remove\n",
    "special_values = ['-9', 'nan', '<40', 'Unspecified', 'Unknown', 'Undetermined', 'Other/-9', 'U', 'M ']\n",
    "\n",
    "# Create a boolean mask to identify rows containing special values\n",
    "special_value_mask = merged_df.isin(special_values)\n",
    "\n",
    "# Remove rows with special values from the DataFrame\n",
    "filtered_df = merged_df[~special_value_mask.any(axis=1)]\n",
    "\n",
    "# Print the shape of the filtered DataFrame\n",
    "print(f'Shape of the filtered DataFrame: {filtered_df.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d058fb-989c-4acf-9288-ebe74971ad1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 24)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfc40df-ce86-433e-b029-214cbfff3a1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the filtered DataFrame as a CSV file\n",
    "filtered_df.to_csv('filtered_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d811b7e8-a26f-4293-b934-846c98ccfa34",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Data export to csv\n",
    "## One. filtered_data is merged dataset with rows containing unique values dropped. \n",
    "### Rows containing '-9', 'NaN', '\"BLANK-EMPTY CELL\"' not removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ebc5d3-fdb4-4299-a21c-c5a251aef206",
   "metadata": {},
   "source": [
    "# Data Processing. \n",
    "## Filter data to last ten years 2013 - 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05e0535-f40d-45da-bbc0-2edc7fac1b97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the 'filtered_df' from the CSV file\n",
    "filtered_df = pd.read_csv('filtered_data.csv')\n",
    "\n",
    "# Convert the 'Year' column to integers\n",
    "filtered_df['Year'] = filtered_df['Year'].astype(int)\n",
    "\n",
    "# Create a new DataFrame with only years between 2013 and 2023\n",
    "filtered_df_2013_to_2023 = filtered_df[(filtered_df['Year'] >= 2013) & (filtered_df['Year'] <= 2023)]\n",
    "\n",
    "# Reset the index if needed\n",
    "filtered_df_2013_to_2023.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the new DataFrame with years 2013 to 2023\n",
    "filtered_df_2013_to_2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc97ab67-73b4-48dd-a29f-8a3a1d0b5ace",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the filtered DataFrame to a new CSV file\n",
    "filtered_df_2013_to_2023.to_csv('filtered_data_with_years_2013_to_2023.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456d5f0e-f3b5-4a65-91ce-ceb08ea36767",
   "metadata": {},
   "source": [
    "## Create dataframes for export to csv to be used as database tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89905a6c-b533-452f-a443-b94967863e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['Crash ID', 'State', 'Month', 'Year', 'Dayweek', 'Time', 'Crash Type',\n",
    "#        'Bus Involvement', 'Heavy Rigid Truck Involvement',\n",
    "#        'Articulated Truck Involvement', 'Speed Limit', 'Road User', 'Gender',\n",
    "#        'Age', 'National Remoteness Areas', 'SA4 Name 2021',\n",
    "#        'National LGA Name 2021', 'National Road Type', 'Christmas Period',\n",
    "#        'Easter Period', 'Age Group', 'Day of week', 'Time of day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc89ec-92af-4043-8501-f74868120dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of columns to include\n",
    "selected_columns = ['Crash ID', 'Year', 'Month', 'Day of week', 'Dayweek', 'Time', 'Christmas Period', 'Easter Period']\n",
    "\n",
    "# Create a new DataFrame with the selected columns\n",
    "date_df = filtered_df_2013_to_2023[selected_columns]\n",
    "\n",
    "# Add a new \"Date ID\" column with values like 'date1', 'date2', 'date3', and so on\n",
    "date_df['Date ID'] = ['date' + str(i) for i in range(1, len(date_df) + 1)]\n",
    "\n",
    "# Reorder the columns with \"Date ID\" on the far left\n",
    "date_df = date_df[['Date ID'] + ['Crash ID', 'Year', 'Month', 'Day of week', 'Dayweek', 'Time', 'Christmas Period', 'Easter Period']]\n",
    "\n",
    "date_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdd4eb5-20fc-40dc-8e08-beb845ce4b58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the filtered DataFrame to a new CSV file\n",
    "date_df.to_csv('date_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee39725-0d89-49cd-9842-61353fe22688",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of columns to include\n",
    "selected_columns = ['Crash ID', 'Crash Type', 'Number of Fatalities', 'Road User', 'Gender', 'Age', 'Age Group']\n",
    "\n",
    "# Create a new DataFrame with the selected columns\n",
    "user_df = filtered_df_2013_to_2023[selected_columns]\n",
    "\n",
    "# Add a new \"Date ID\" column with values like 'date1', 'date2', 'date3', and so on\n",
    "user_df['User ID'] = ['date' + str(i) for i in range(1, len(date_df) + 1)]\n",
    "\n",
    "# Reorder the columns with \"Date ID\" on the far left\n",
    "user_df = date_df[['User ID'] + ['Crash ID', 'Crash Type', 'Number of Fatalities', 'Road User', 'Gender', 'Age', 'Age Group']]\n",
    "\n",
    "user_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3712be38-da91-4e98-a90b-2d19bde9ce26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the filtered DataFrame to a new CSV file\n",
    "user_df.to_csv('user_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555300ea-ecf2-4d57-b8f2-9d2b99ec5906",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of columns to include\n",
    "selected_columns = ['Crash ID', 'Bus Involvement', 'Heavy Rigid Truck Involvement', 'Articulated Truck Involvement']\n",
    "\n",
    "# Create a new DataFrame with the selected columns\n",
    "HGV_df = filtered_df_2013_to_2023[selected_columns]\n",
    "\n",
    "# Add a new \"Date ID\" column with values like 'date1', 'date2', 'date3', and so on\n",
    "HGV_df['HGV ID'] = ['date' + str(i) for i in range(1, len(date_df) + 1)]\n",
    "\n",
    "# Reorder the columns with \"Date ID\" on the far left\n",
    "HGV_df = date_df[['HGV ID'] + ['Crash ID', 'Bus Involvement', 'Heavy Rigid Truck Involvement', 'Articulated Truck Involvement']]\n",
    "\n",
    "HGV_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6609cce-8761-42a2-b745-c9ea47fbaef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the filtered DataFrame to a new CSV file\n",
    "HGV_df.to_csv('HGV_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01acd756-b5ac-4fed-9a6a-5f38fed759ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Crash_df and add the \"ID\" columns to the Crash_df [ Date_ID, User_ID, HGV_ID]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da084388-306b-4133-b4e8-8342b01d2d52",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fff252-8a14-48b8-af31-c3253e4632e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 'filtered_df' from the CSV file\n",
    "filtered_df_2013_to_2023_df = pd.read_csv('filtered_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d8a19f-b065-4f01-97e0-f0635be7527b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72b2e2-14bd-4249-a509-f81a93fcd476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below analysis was performed on 'merged_df' so change merged_df to filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e6cce-fe2c-4b54-bc4c-c3725cc556d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a histogram of ages\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(merged_df['Age'], bins=20)\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# NOTE # -9 represents a NaN and is creating the below '0' results, -9 will be cleaned out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06632b2c-1044-4d8a-88f8-603a405b1cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What age group has the most likely car accidents each year?\n",
    "age_group_yearly = merged_df.groupby(['Year', 'Age Group'])['Crash ID'].count().unstack()\n",
    "age_group_yearly.plot(kind='bar', stacked=True)\n",
    "plt.title('Age Group with Most Car Accidents Each Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# NOTE # -9 represents a NaN and will be cleaned out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bdd44b-44ec-4223-b60e-a25386460697",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fatalities over years\n",
    "fatalities_over_years = merged_df.groupby('Year')['Number of Fatalities'].sum()\n",
    "fatalities_over_years.plot(kind='line')\n",
    "plt.title('Fatalities Over the Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36003ff-ac25-4b18-a3d8-f9c595a8c6c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fatalities per month\n",
    "fatalities_per_month = merged_df.groupby('Month')['Number of Fatalities'].sum()\n",
    "fatalities_per_month.plot(kind='bar')\n",
    "plt.title('Fatalities per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3778c7b3-5f6a-4cb4-a2a2-d0127947d4e8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Map month numbers to month names\n",
    "fatalities_per_month.index = fatalities_per_month.index.map(lambda x: calendar.month_name[x])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "fatalities_per_month.plot(kind='bar')\n",
    "plt.title('Fatalities per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e424df3f-83f8-4b34-b1b5-181dafaf4e1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of fatalities over the years for each state\n",
    "# Create a line plot for fatality trends by state over the past decade\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.lineplot(data=merged_df, x='Year', y='Number of Fatalities', hue='State')\n",
    "plt.title('Fatality Trends Over the Past Decade by State')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Fatalities')\n",
    "plt.legend(title='State')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70dce9f-9eec-4479-a72f-678ccb8cad5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a line plot that focuses only on the years after 2020.\n",
    "plt.figure(figsize=(12, 8))\n",
    "recent_years_df = merged_df[merged_df['Year'] >= 2020]\n",
    "sns.lineplot(data=recent_years_df, x='Year', y='Number of Fatalities', hue='State')\n",
    "plt.title('Fatality Trends After 2020 by State')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Fatalities')\n",
    "plt.legend(title='State')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46ce9e8-1d41-4842-a0d2-6ee16f2bd87d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Highest fatality per state\n",
    "state_fatality = merged_df.groupby('State')['Number of Fatalities'].sum().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "state_fatality.plot(kind='bar')\n",
    "plt.title('Fatalities by State')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Total Fatalities')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a342dd4-1b52-40f8-b3a0-00324480b833",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create separate line plots for NT and Tas\n",
    "plt.figure(figsize=(12, 8))\n",
    "nt_df = merged_df[merged_df['State'] == 'NT']\n",
    "sns.lineplot(data=nt_df, x='Year', y='Number of Fatalities', label='NT')\n",
    "tas_df = merged_df[merged_df['State'] == 'Tas']\n",
    "sns.lineplot(data=tas_df, x='Year', y='Number of Fatalities', label='Tas')\n",
    "plt.title('Fatality Trends in NT and Tas Over the Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Fatalities')\n",
    "plt.legend(title='State')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d3c07e-846c-4bc1-bcfe-9d54330a9441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a bar chart to show the number of crashes by road type\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=merged_df, y='National Road Type')\n",
    "plt.title('Number of Crashes by Road Type')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Road Type')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74de6395-048d-4124-88ed-17043fef5f60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a bar chart to show the distribution of accidents by time of day\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=merged_df, y='Time of Day')\n",
    "plt.title('Accidents by Time of Day')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Time of Day')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a6f509-0e35-492d-86e4-74866b8117fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a bar chart to show the distribution of accidents during Christmas and Easter periods\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=merged_df, x='Christmas Period')\n",
    "plt.title('Accidents During Christmas Period')\n",
    "plt.xlabel('Christmas Period')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=merged_df, x='Easter Period')\n",
    "plt.title('Accidents During Easter Period')\n",
    "plt.xlabel('Easter Period')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ad8bfa-e4ac-41c5-8a81-af30c5414340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fatalities by Speed Limit\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=merged_df, x='Speed Limit', order=merged_df['Speed Limit'].value_counts().index)\n",
    "plt.title('Fatalities by Speed Limit')\n",
    "plt.xlabel('Speed Limit')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916011ae-ef6e-459d-b66b-83f1b96a205f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
